{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Nested Cross-Validation\n",
    "\n",
    "**Instructions:**\n",
    "* go through the notebook and complete the **tasks** .  \n",
    "* Make sure you understand the examples given!\n",
    "* When a question allows a free-form answer (e.g., ``what do you observe?``) create a new markdown cell below and answer the question in the notebook.\n",
    "* ** Save your notebooks when you are done! **\n",
    "\n",
    "In the previous lab, we looked at cross-validation when the parameters of our classifier (e.g., k-NN) where known.\n",
    "In this lab, you will be extending the code for cross-validation in order to find the best parameters to use for each fold, by using a validation set.  Please have a look at the relevant lecture slides that demo how to apply nested cross-validation in order to remember the procedure.\n",
    "\n",
    "**Note** You can always copy the code in a separate notebook (or, a plain text file .py that you can run with python from the command line) if you want.  After you are done, you can copy the code back in this notepad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> Run the cell below to load our data. Note that besides adding noise, we also initialize the numpy random seed - this is in order to always get the same results regardless of how many times we run the code. Otherwise, this piece of code is the same as the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import k-nn classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#view a description of the dataset (uncomment next line to do so)\n",
    "#print(iris.DESCR)\n",
    "\n",
    "#Set X equal to features, Y equal to the targets\n",
    "\n",
    "X=iris.data \n",
    "y=iris.target \n",
    "\n",
    "\n",
    "mySeed=1234567\n",
    "#initialize random seed generator \n",
    "np.random.seed(mySeed)\n",
    "\n",
    "#we add some random noise to our data to make the task more challenging\n",
    "X=X+np.random.normal(0,0.5,X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> Your task is now to write your own nested cross-validation function.\n",
    "\n",
    "You can assume that we want to run 5-fold cross-validation, and evaluate the number of neighbours (from 1 to 10 inclusive), along with the 'euclidean' and 'manhattan' distances.\n",
    "\n",
    "Your function should split the data (using indexes) into appropriate bins, similarly to how this was done in the previous lab. \n",
    "\n",
    "For each fold, the testing set should consist of indices in one bin, the validation set should consist of indices in another bin, and the rest of the bins can be assigned to your training set.\n",
    "\n",
    "Subsequently, we loop through all different parameters (one for loop for neighbours, one for loop for distances), train on the training set and test on the validation set.\n",
    "\n",
    "Once we are done, we have the best performing set of parameters on our validation set.  We subsequently merge the training set with the validation set, and train on that set using best parameters.\n",
    "\n",
    "Finally, we evaluate on our test set, and proceed to the next fold.\n",
    "\n",
    "Your function should return the accuracies on the test set (with best parameters) over all five folds, e.g. ``[0.80000000000000004, 0.8666666666666667, 0.80000000000000004, 0.96666666666666667, 0.73333333333333328]``\n",
    "\n",
    "The code below is commented so that you can work through developing the function - if you feel more comfortable, you can start working on this code in a different cell/ide and then copy the code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euclidean\n",
      "euclidean\n",
      "1\n",
      "0.8666666666666667\n",
      "euclidean\n",
      "2\n",
      "0.8666666666666667\n",
      "euclidean\n",
      "3\n",
      "0.8333333333333334\n",
      "euclidean\n",
      "4\n",
      "0.8666666666666667\n",
      "euclidean\n",
      "5\n",
      "0.8666666666666667\n",
      "euclidean\n",
      "6\n",
      "0.8666666666666667\n",
      "euclidean\n",
      "7\n",
      "0.8333333333333334\n",
      "euclidean\n",
      "8\n",
      "0.8333333333333334\n",
      "euclidean\n",
      "9\n",
      "0.8333333333333334\n",
      "euclidean\n",
      "10\n",
      "0.8333333333333334\n",
      "manhattan\n",
      "manhattan\n",
      "1\n",
      "0.8333333333333334\n",
      "manhattan\n",
      "2\n",
      "0.8666666666666667\n",
      "manhattan\n",
      "3\n",
      "0.8333333333333334\n",
      "manhattan\n",
      "4\n",
      "0.8666666666666667\n",
      "manhattan\n",
      "5\n",
      "0.8666666666666667\n",
      "manhattan\n",
      "6\n",
      "0.8666666666666667\n",
      "manhattan\n",
      "7\n",
      "0.8333333333333334\n",
      "manhattan\n",
      "8\n",
      "0.8333333333333334\n",
      "manhattan\n",
      "9\n",
      "0.8333333333333334\n",
      "manhattan\n",
      "10\n",
      "0.8333333333333334\n",
      "** End of val for this fold, best NN 1 best Dist euclidean\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-eddd8f5aa731>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;31m#call your nested crossvalidation function:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m \u001b[0maccuracy_fold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmyNestedCrossVal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'euclidean'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'manhattan'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmySeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-eddd8f5aa731>\u001b[0m in \u001b[0;36mmyNestedCrossVal\u001b[1;34m(X, y, foldK, nns, dists, mySeed)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0maccuracy_fold\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfoldNum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmyAccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfoldTest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'==== Final Cross-val on test on this fold with NN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbestNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dist'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbestDistance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' accuracy '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfoldTest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m#DEBUG: Using old KNN to see if I've messed up anything\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# nested cross validation function\n",
    "# X - data / features\n",
    "# y - outputs\n",
    "# foldK - number of folds\n",
    "# nns - list of number of neighbours parameter for validation\n",
    "# dists - list of distances for validation\n",
    "# mySeed - random seed\n",
    "# returns: accuracy over 5 folds (list)\n",
    "\n",
    "#n_neighbors=5, metric='euclidean'\n",
    "def knnise(training,labels,test,neighbours,myMetric):\n",
    "    knn=KNeighborsClassifier(n_neighbors=neighbours, metric=myMetric)\n",
    "    #define training and testing data, fit the classifier\n",
    "    knn.fit(training,labels)\n",
    "    #predict values for test data based on training data\n",
    "    k_non=knn.predict(test)\n",
    "    return k_non;\n",
    "\n",
    "def myAccuracy(testing,predicted):\n",
    "    mistakes=0\n",
    "    for i in range (len(testing)):\n",
    "        if (testing[i]!=predicted[i]): mistakes+=1\n",
    "    return 1-mistakes/len(testing);\n",
    "\n",
    "def myNestedCrossVal(X,y,foldK,nns,dists,mySeed):\n",
    "    np.random.seed(mySeed)\n",
    "    accuracy_fold=np.zeros(foldK)\n",
    "    \n",
    "    #TASK: use the function np.random.permutation to generate a list of shuffled indices from in the range (0,number of data)\n",
    "    #(you did this already in a task above)\n",
    "    indices=np.random.permutation(np.arange(len(X)))\n",
    "    #print(indices)\n",
    "    \n",
    "    #TASK: use the function array_split to split the indices to foldK different bins (here, 5)\n",
    "    #uncomment line below\n",
    "    bins=np.split(indices,foldK)\n",
    "    #print(bins)\n",
    "    \n",
    "    #no need to worry about this, just checking that everything is OK\n",
    "    assert(foldK==len(bins))\n",
    "    \n",
    "    #loop through folds\n",
    "    for foldNum in range(0,foldK):\n",
    "        foldTest=bins[foldNum%foldK]  # list to save current indices for testing\n",
    "        foldVal=bins[(foldNum+1)%foldK]    # list to save current indices for validation\n",
    "        #loop through all bins, take bin i for testing, the next bin for validation and the rest for testing\n",
    "        foldTrain=np.delete(bins,[foldNum%foldK,(foldNum+1)%foldK],0).flatten()    # list to save current indices for training\n",
    "        '''\n",
    "        print (foldTrain)\n",
    "        print (foldTest)\n",
    "        print (foldVal)\n",
    "        print (bins)\n",
    "        '''\n",
    "        \n",
    "\n",
    "        '''   \n",
    "        print('** Train', len(foldTrain), foldTrain)\n",
    "        print('** Val', len(foldVal), foldVal)\n",
    "        print('** Test', len(foldTest), foldTest)\n",
    "        '''\n",
    "        \n",
    "        #no need to worry about this, just checking that everything is OK\n",
    "        assert not np.intersect1d(foldTest,foldVal)\n",
    "        assert not np.intersect1d(foldTrain,foldTest)\n",
    "        assert not np.intersect1d(foldTrain,foldVal)\n",
    "       \n",
    "        #'''\n",
    "        bestDistance='' #save the best distance metric here\n",
    "        bestNN=-1 #save the best number of neighbours here\n",
    "        bestAccuracy=-10 #save the best attained accuracy here (in terms of validation)      \n",
    "        # loop through all parameters (one for loop for distances, one for loop for nn)\n",
    "        for distLoop in range (0,len(dists)):\n",
    "            #print (dists[distLoop])\n",
    "            for neighLoop in range (0,len(nns)):\n",
    "                # train the classifier on current number of neighbours/distance\n",
    "                val_pred=knnise(X[foldTrain],y[foldTrain],X[foldVal],nns[neighLoop],dists[distLoop])\n",
    "                # obtain results on validation \n",
    "                currentAccuracy=myAccuracy(y[foldVal],val_pred)\n",
    "                '''\n",
    "                print (dists[distLoop])\n",
    "                print (nns[neighLoop])\n",
    "                print (currentAccuracy)\n",
    "                '''\n",
    "                # save parameters if results are the best we had\n",
    "                if (currentAccuracy>bestAccuracy): \n",
    "                    bestAccuracy=currentAccuracy\n",
    "                    bestDistance=dists[distLoop] \n",
    "                    bestNN=nns[neighLoop]\n",
    "        print('** End of val for this fold, best NN', bestNN, 'best Dist', bestDistance)\n",
    "        #'''\n",
    "        \n",
    "        #evaluate on test data:\n",
    "        #extend your training set by including the validation set             \n",
    "        foldTrain=np.concatenate((foldTrain,foldVal),0)\n",
    "        #train k-NN classifier on new training set and test on test set\n",
    "        test_pred=knnise(X[foldTrain],y[foldTrain],X[foldTest],bestNN,bestDistance)\n",
    "        #get performance on fold, save result in accuracy_fold array\n",
    "        accuracy_fold[foldNum]=myAccuracy(y[foldTest],test_pred)\n",
    "        \n",
    "        print('==== Final Cross-val on test on this fold with NN', bestNN, 'dist', bestDistance, ' accuracy ',accuracy_score(y[foldTest],y_pred))\n",
    "        \n",
    "        #DEBUG: Using old KNN to see if I've messed up anything\n",
    "        '''\n",
    "        knn=KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "        #define training and testing data, fit the classifier\n",
    "        knn.fit(X[foldTrain],y[foldTrain])\n",
    "        #predict values for test data based on training data\n",
    "        y_mypred=knn.predict(X[foldTest])\n",
    "        \n",
    "        \n",
    "        #print(foldNum)\n",
    "        #print(accuracy_fold)\n",
    "        #accuracy_fold[foldNum]=1-mistakes/len(foldTest)\n",
    "        accuracy_fold[foldNum]=myAccuracy(y[foldTest],y_mypred)\n",
    "        '''\n",
    "        \n",
    "    return accuracy_fold;\n",
    "    \n",
    "#call your nested crossvalidation function:\n",
    " \n",
    "accuracy_fold=myNestedCrossVal(X,y,5,list(range(1,11)),['euclidean','manhattan'],mySeed)\n",
    "\n",
    "print(accuracy_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
